<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>On Bayesian and frequentist, latent variables and parameters | Dustin Tran</title>
	<meta name="description" content="I’ve been helping write tutorials that teach concepts such as black box variational inference in Edward. And as I’ve been editing, I’ve noticed the majority ...">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/on-bayesian-and-frequentist-latent-variables-and-parameters">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">On Bayesian and frequentist, latent variables and parameters</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Jul 3, 2016</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><p>I’ve been helping write tutorials that teach concepts such as <a href="https://github.com/blei-lab/edward/pull/136">black box variational inference</a> in Edward. And as I’ve been editing, I’ve noticed the majority of my suggestions are about the writing rather than the code. Communication is important—arguably more important in papers than the idea itself. Communication evokes different lines of thinking about how to approach problems, and it always has subtleties regarding the different culture and  set of problems one finds important. Here are a few subtleties that have come up recently and my stock answers to them.</p>

<ol>
  <li>
    <p><strong>“Bayesian models” framed as placing priors on parameters of a likelihood.</strong> This is a very frequentist -&gt; Bayesian line of thinking.  I strongly believe models should simply be framed as a joint distribution <script type="math/tex">p(x, z)</script> for data <script type="math/tex">x</script> and latent variables <script type="math/tex">z</script>.<sup>1</sup> The other line of thinking can lead to misunderstandings about Bayesian analysis. For example, a classical argument (and one which I still hear among some of my statistician friends in Berkeley and Stanford) is that Bayesian analysis is subjective because the prior <script type="math/tex">p(z)</script> is subjective. But in fact the entire model is subjective—both likelihood and prior as <a href="http://andrewgelman.com/2015/08/25/can-you-change-your-bayesian-prior/">Andrew repeatedly states</a>. The distinction away from the fact that you’re specifying a model is often meaningless. This relates to the old adage that “all models are wrong” (Box, 1976). Any model you work with is “subjective” according to the assumptions you’re willing to make in the model.</p>
  </li>
  <li>
    <p><strong>Bayesian versus frequentist models.</strong> The secret that no one wants to say publicly is that there’s no difference! A “latent variable model” or “hierarchical model” or “generative model” in Bayesian literature is just a “random effects model” in frequentist literature. I try to avoid attaching specific statistical methodologies to models because there is no difference. They are all just “probabilistic models”. The statistical methodology—whether it be Bayesian, frequentist, fiducial, whatever—is about how to reason with the model given data. That is, they are about “inference” and not the “model”. Why else do we do EM, a frequentist tool, on hierarchical models, typically labeled a Bayesian tool?</p>

    <p>Frequentists may often use only the likelihood <script type="math/tex">p(x \mid z)</script> as the model, so it is trivially a joint distribution <script type="math/tex">p(x, z)</script> with a point mass distribution for <script type="math/tex">z</script>. But it’s useful to keep in mind the joint distribution as it explicitly bakes in the modeling assumptions one makes.</p>
  </li>
  <li>
    <p><strong>Latent variables vs parameters.</strong> These days I try to stick to calling <script type="math/tex">z</script> latent variables rather than parameters (and hence why I prefer to use <script type="math/tex">z</script> rather than <script type="math/tex">\theta</script>). Parameters connote the idea of having only one setting, and it brings up the whole frequentist-Bayesian debacle about whether parameters can be random. Calling them latent variables instead simply state that the model uses random variables that remain unobserved during inference. We may be interested in inferring the latent variables, and thus it is natural to think about what <script type="math/tex">p(z \mid x)</script> means, or what it means to approximate <script type="math/tex">p(z \mid x)</script> with a point. Or we may not be directly interested in the latent variables, and thus it’s natural to integrate over them during inference.<sup>2</sup> Somehow I find this less convincing when thinking about <script type="math/tex">z</script> as “parameters”. “Parameters” introduce a concept beyond random variables and their realizations, and which I can’t formally wrap my head around.<sup>3</sup></p>
  </li>
  <li>
    <p><strong>Prior belief vs prior information.</strong> I’ve started to follow Andrew in using the latter and avoiding the former. <a href="http://andrewgelman.com/2015/07/15/prior-information-not-prior-belief/">Prior belief makes Bayesian analysis sound like a religion</a>. Prior information comes not only in the context of prior information about the latent variables but prior information about the problem (e.g., data’s likelihood) in general.</p>
  </li>
</ol>

<p><sup>1</sup> For clarity, I’m ignoring the nuance with probabilistic programs, in the same way we typically don’t communicate everything using measures, or by first describing the <a href="https://en.wikipedia.org/wiki/Category_theory">category</a> we’re working in.</p>

<p><sup>2</sup> Random effects (Eisenhart, 1947), incidental parameters (Neyman and Scott, 1948), nuisance parameters (Kalbfleisch and Sprott, 1970), and latent data (Rubin, 1976) are often explained in frequentist literature as something to integrate over. But I don’t like them because they don’t explain what it means to treat them as random variables.</p>

<p><sup>3</sup> In practice, we often work with “parameters” in the sense that we often work with point estimates during inference. This doesn’t mean that we shouldn’t continue working with them—it’s just that in principle it’s better to think about the optimal thing (inferring the full distribution) and viewing the point estimate as an approximation to the optimal thing. That is, what we should do in principle and what we should do in practice.</p>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
