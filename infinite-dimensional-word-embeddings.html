<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Infinite Dimensional Word Embeddings | Dustin Tran</title>
	<meta name="description" content="  Eric Nalisnick and Sachin Ravi. Infinite Dimensional Word Embeddings. arXiv preprint arXiv:1511.05392, 2015.">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/infinite-dimensional-word-embeddings">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">Infinite Dimensional Word Embeddings</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Nov 28, 2015</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><ul>
  <li>Eric Nalisnick and Sachin Ravi. Infinite Dimensional Word Embeddings. <a href="http://arxiv.org/abs/1511.05392">arXiv preprint arXiv:1511.05392</a>, 2015.</li>
</ul>

<h2 id="summary">Summary</h2>

<p>Word embeddings have been huge for the NLP community ever since Tomas Mikolov’s 2013 paper (it’s gotten over 1000 citations in 2 years!). The basic idea is to learn a <script type="math/tex">z</script>-dimensional parameter vector <script type="math/tex">w_i\in\mathbb{R}^z</script> associated to each word in a vocabulary. This is important as part of a machine learning pipeline which extracts features for language models, text classification, etc. Not unlike advertisements of discrete Bayesian nonparametric models, Nalisnick and Ravi propose the Infinite Skip-Gram model, which allows the dimension <script type="math/tex">z</script> of the word vectors to grow arbitrarily.</p>

<p>Letting word vector <script type="math/tex">w_i\in\mathbb{R}^\infty</script>, context vector <script type="math/tex">c_k\in\mathbb{R}^\infty</script>, and random positive integer <script type="math/tex">z\in\mathbb{Z}^+</script>, they consider multinomial logistic regression with (Eq.3)</p>

<script type="math/tex; mode=display">p(w_i,c_k,z) = \frac{1}{Z} e^{-E(w_i,c_k,z)},</script>

<p>where <script type="math/tex">Z</script> is the partition function summing over all <script type="math/tex">w_i,c_k,z</script>. This is the same as the original model for word embeddings, except now there is an auxiliary variable <script type="math/tex">z</script> and the vectors are infinite-dimensional. This seems intractable given some energy function <script type="math/tex">E(w_i,c_k,z)</script>, as the partition function requires a summation over countably infinite values of <script type="math/tex">z</script>. However, similar to Côté and Larochelle (2015), they show that the following penalized energy function enables finite computation (Eq.4):</p>

<script type="math/tex; mode=display">E(w_i,c_k,z) = -\sum_{j=1}^z w_{i,j} c_{k,j} + \Big[z\log a -\sum_{j=1}^z (w_{i,j}^2 +  c_{k,j}^2)\Big].</script>

<p>The first term is the original (unpenalized) energy function, which is the inner product of <script type="math/tex">z</script>-dimensional vectors. The second term, the penalty, consists of two expressions: the <script type="math/tex">L_2</script> penalty forces the word and context vectors to be zero after sufficiently large index, and the <script type="math/tex">z\log a</script> term forces a convergent geometric series for the denominator of the conditional distribution <script type="math/tex">p(z\mid w, c)</script> (Eq.5). Tractability of the conditional distribution is required during optimization.</p>

<p>For inference, they treat <script type="math/tex">z</script> as a nuisance parameter and perform an approximate maximum likelihood estimation. Specifically, they minimize an upper bound to the negative log-likelihood <script type="math/tex">\mathcal{L}=-\log \int p(c,z\mid w)~\mathrm{d}z</script> over the word vectors <script type="math/tex">w</script>. Similar to how word embeddings require negative sampling for computational efficiency, Nalisnick and Ravi make additional Monte Carlo approximations, such as evaluating Monte Carlo estimates of the gradient of (an upper bound to) this objective.</p>

<h2 id="discussion">Discussion</h2>

<p>Very cool idea! I’m always a fan of infinite-dimensional models, not just because they’re mathematically interesting but because there’s no reason that word vectors should be restricted to a fixed dimensionality. In a streaming setting, for example, the word representations should grow increasingly more complex as more information in the data is available.</p>

<p>Although they don’t explicitly state this, what they’re really doing is applying a variational lower bound for inference, invoking Jensen’s inequality and setting the variational distribution as the prior <script type="math/tex">~q(z)= p(z\mid w_i)</script> (Eq.7). If readers are familiar with the <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">score function estimator</a> used for variational inference (Paisley et al., 2012), the Monte Carlo gradient in Eq.10 applies the same trick. Following this interpretation, it surprises me then that the algorithm works so well. Using model priors for variational approximations make for quite a loose lower bound of the log-likelihood.</p>

<p>I highly recommmend reading this paper. It’s very well-written and easy to read even if you don’t know anything about word embeddings. The related work section places it very appropriately in context. They mention Vilnis and McCallum (2015) a lot, and I’d be very interested to see if they can extend their work to inferring infinite-dimensional distributions of words rather than infinite-dimensional point-estimates of words.</p>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
