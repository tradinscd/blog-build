<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Adversarial autoencoders | Dustin Tran</title>
	<meta name="description" content="  Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial Autoencoders. arXiv preprint arXiv:1511.05644, 2015.">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/adversarial-autoencoders">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">Adversarial autoencoders</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Nov 29, 2015</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><ul>
  <li>Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial Autoencoders. <a href="http://arxiv.org/abs/1511.05644">arXiv preprint arXiv:1511.05644</a>, 2015.</li>
</ul>

<h2 id="summary">Summary</h2>

<p>Following the trend in auto-encoders for generative modelling, Makhzani et al. propose an adversarial version using the framework of generative adversarial networks (Goodfellow et al., 2014). I have a bit of trouble following the terminology in these lines of works, so in my summary I will use different notation from the papers, following statistical conventions which I personally find more elucidating. And I’ll motivate it differently, coming from various divergence measure arguments.</p>

<p>I’ll first review generative adversarial networks (GANs). Suppose we observe data <script type="math/tex">x</script>. GANs employ a pair of models <script type="math/tex">(G,D)</script>, where <script type="math/tex">G</script> is the generative model and <script type="math/tex">D</script> is a discriminative model to help perform inference on <script type="math/tex">G</script>.</p>

<ul>
  <li>The discriminative model <script type="math/tex">D</script> aims to output high probability for samples <script type="math/tex">x</script> which come from the empirical data distribution <script type="math/tex">p_{\text{data}}(x)</script>, and output low probability for samples <script type="math/tex">x</script> which come from the generative model <script type="math/tex">p_{\text{g}}(x)</script>.</li>
  <li>The generative model <script type="math/tex">G</script> aims to fool the discriminative model, i.e., it consists of a prior <script type="math/tex">p(z)</script> and likelihood <script type="math/tex">p(x\mid z)</script>, and it aims to generate data most closely matching the observed data.</li>
</ul>

<p>The objective follows a minimax game</p>

<script type="math/tex; mode=display">\min_G
\max_D\,
\mathbb{E}_{p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{p_{\text{g}}(x)}[\log (1- D(x))],</script>

<p>which alternates between learning the discriminator <script type="math/tex">D</script> and learning the generator <script type="math/tex">G</script>. The intuition is that finding the optimal <script type="math/tex">p_{\text{g}}(x)</script> among a class of generative models <script type="math/tex">G</script> corresponds to minimizing the Jensen-Shannon divergence</p>

<script type="math/tex; mode=display">JSD(p_{\text{data}}(x) \| p_{\text{g}}(x)) = \frac{1}{2}\mathrm{KL}\Big(p_{\text{data}} \Big\| \frac{p_{\text{data}} + p_{\text{g}}}{2}\Big) +
\frac{1}{2}\mathrm{KL}\Big(p_{\text{g}} \Big\| \frac{p_{\text{data}} + p_{\text{g}}}{2}\Big).</script>

<p>Minimizing <script type="math/tex">JSD</script> directly is not possible; this is where the objective above, using an additional discriminative model, comes in handy. Recall also that maximum likelihood estimation minimizes <script type="math/tex">\mathrm{KL}(p_{\text{data}} \| p_{\text{g}})</script> (White, 1982). Using JSD leads to a more balanced approximation not underfitting or overfitting modes. It’s also worth noting that unlike approximate posterior inference, these are divergence measures directly on data distributions for <script type="math/tex">x</script>, not on latent variable distributions for <script type="math/tex">z</script>. For example, variational inference minimizes <script type="math/tex">\mathrm{KL}(q(z) \| p(z\mid x))</script>.</p>

<p>In adversarial auto-encoders, we go back to minimizing divergence measures on the latent variables. Recall that the variational auto-encoder’s objective (Kingma and Welling, 2014) is equivalently</p>

<script type="math/tex; mode=display">\min_q\, \mathbb{E}_{q(z\mid x)}[-\log p(x\mid z)] + \mathrm{KL}(q(z\mid x)\|p(z)),</script>

<p>where the first term is the reconstruction error—with the decoder <script type="math/tex">p(x\mid z)</script> evaluating codes from the encoder <script type="math/tex">q(z\mid x)</script>—and the second term is the regularizer. The adversarial auto-encoder’s objective simply changes the regularizer to <script type="math/tex">JSD(p(z)\|q(z))</script>, where we define the <em>aggregated posterior</em></p>

<script type="math/tex; mode=display">q(z) = \int q(z\mid x) p_{\text{data}}(x)\mathrm{d}x.</script>

<p><script type="math/tex">JSD(p(z)\|q(z))</script> is just as intractable as minimizing <script type="math/tex">JSD</script> in the data space, so it requires an adversarial network: <script type="math/tex">p(z)</script> replaces the original <script type="math/tex">p_{\text{data}}(x)</script> and <script type="math/tex">q(z\mid x)</script> replaces the original <script type="math/tex">p_{\text{g}}(x)</script>:</p>

<script type="math/tex; mode=display">\min_G
\max_D\,
\mathbb{E}_{p(z)}[\log D(z)] + \mathbb{E}_{q(z\mid x)}[\log (1- D(z))].</script>

<p>To train adversarial autoencoders, one alternates between minimizing the reconstruction error of the auto-encoder, i.e., <script type="math/tex">\mathbb{E}_{q(z\mid x)}[-\log p(x\mid z)]</script>, and training the adversarial network. The rest of the paper goes into standard experiments for semi-supervised learning on MNIST and SVHN. They also comment on an extension of generative moment matching networks (Li et al., 2015) to auto-encoders, also following these ideas.</p>

<h2 id="discussion">Discussion</h2>

<p>I see generative adversarial networks as fascinating inference algorithms because they’re a rare case where we can minimize a more general divergence measure between the generative model and the data distribution. It makes sense that one should avoid MLE’s KL objective, which can lead to overdispersed distributions, in favor of this approach. And it’s great to see attempts at trying to port these same ideas to the variational inference/auto-encoder setting.</p>

<p>The experiments seem to show that adversarial auto-encoders lead to higher visual fidelity than variational auto-encoders. It’s not clear to me  why this is the case however. The fact that there’s even a KL term as a regularizer in the VAE objective is a simple byproduct of the variational lower bound. In the end, it is still minimizing <script type="math/tex">\mathrm{KL}(q(z\mid x)\|p(z\mid x))</script>. However, replacing the KL regularizer with <script type="math/tex">JSD</script> makes it unclear what the underlying divergence measure is any longer. I buy that <script type="math/tex">JSD</script> ensures less “holes” as <script type="math/tex">q(z)</script> approximates the prior <script type="math/tex">p(z)</script>, but there’s a weird conflict as <script type="math/tex">q</script> simultaneously gets trained when minimizing the reconstruction error. The underlying divergence measure, if it exists, is most likely not as simple as <script type="math/tex">JSD(p(z\mid x)\|q(z\mid x))</script>—although if it were then that would be fantastic(?). Does this alternating procedure also necessarily converge?</p>

<p>Obligatory comment in relation to hierarchical variational models (Ranganath et al., 2015): Their “universal approximator posterior”, as a possible choice for the encoder <script type="math/tex">q(z\mid x)</script>, is a hierarchical variational model. To wield the intractability of the density they naively apply Monte Carlo estimates. We’ve found this not to work well in practice due to very high variance, forcing a more complicated inference procedure which we outline in our paper.</p>

<p><em>edit</em> (12/1/15): A previous version of this post wrote <script type="math/tex">\mathrm{JSD}(p(z)\|q(z\mid x))</script> instead of <script type="math/tex">\mathrm{JSD}(p(z)\|q(z))</script>. Thanks to Alireza Makhzani for the correction.</p>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
