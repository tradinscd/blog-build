<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Comments on Mark Schmidt's ICML tutorial for convex optimization | Dustin Tran</title>
	<meta name="description" content="Yesterday was a series of tutorials at ICML, and the ones I attended were all quite insightful—for instance, Emily Fox’s coherent map of Bayesian models of t...">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/comments-on-mark-schmidts-icml-tutorial-for-convex-optimization">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">Comments on Mark Schmidt's ICML tutorial for convex optimization</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Jul 7, 2015</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><p>Yesterday was a series of tutorials at ICML, and the ones I attended were all quite insightful—for instance, <a href="http://www.stat.washington.edu/~ebfox/">Emily Fox’s</a> coherent map of Bayesian models of time series analysis (and it was the first time I saw use of a hyper Markov prior in practice!).</p>

<p><a href="http://www.cs.ubc.ca/~schmidtm/">Mark Schmidt</a> presented one on convex optimization, which I commented on briefly to a circle of friends. As I’ve started to repeat many of the same points, I decided to just put them up here in more detail. It was a great talk from the ground up in the canonical point of view of optimization, in which one lays out the assumptions required in order to achieve better convergence rates. All theory in optimization is generally in the regime of Lipschitz continuity, (strong) convexity, and linear differentiable functions. All bets are off when one wanders out of this set of assumptions and only empirical studies can really be made.</p>

<p>There is a lot of interest lately in SAG (Le Roux et al., 2012) and SVRG (Johnson and Zhang, 2013), where their main claim to fame from other stochastic gradient methods is linear convergence rate for strongly convex functions. Stochastic gradient methods in general can only achieve sublinear convergence at best. There was also mention of a few other promising methods, e.g., ASGD from Polyak and Juditsky (1992) which averages the iterates in order to achieve asymptotic optimality, and dual methods which perform optimization on the dual of the objective function. Here are some comparisons of these common stochastic gradient methods:</p>

<h3 id="sag">SAG</h3>
<ul>
  <li>+: linear convergence rate for strongly convex functions</li>
  <li>-: must store memory of gradient (only heuristics exist to compensate for this), can’t work for streaming data, no work on asymptotics</li>
</ul>

<h3 id="svrg">SVRG</h3>
<ul>
  <li>+: linear convergence rate for strongly convex functions</li>
  <li>-: hyperparameter for inner loop, can’t work for streaming data, no work on asymptotics</li>
</ul>

<h3 id="asgd">ASGD</h3>
<ul>
  <li>+: asymptotically efficient, works for streaming data</li>
  <li>-: sublinear convergence rate <script type="math/tex">O(1/t)</script> for strongly convex functions</li>
</ul>

<p>There’s a footnote here on the advertised linear convergence rate for strongly convex functions however. It is a bit misleading as it is a convergence rate to the <em>maximum likelihood estimate</em>, <strong>not</strong> the ground truth. The maximum likelihood estimate asymptotically converges to the truth at the sublinear rate <script type="math/tex">O(1/t)</script>, so if the goal is to recover the ground truth then such methods do not help.</p>

<p>This relatively unknown fact more generally addresses the widespread problem in the optimization community in that few consider the statistical properties when viewing these methods as estimators. Ironically, stochastic gradient descent was originally developed under the statistical theory of stochastic approximations by Robbins and Monro (1951), and it has seen little exposure since then. Convergence rates, which are the primary focus of the optimization community, is simply analysis of the convergence rate of the asymptotic bias of these estimators. It is natural to consider other properties, e.g., the asymptotic variance of the stochastic gradient estimators so as to minimize it.</p>

<p>Moreover, the theoretical results are violated for more complicated models in which the objective function is either nonconvex (e.g., in Bayesian inference using variational methods for mixture models) or nonlinear (e.g., MLE for many statistical models). While it’s difficult to make any theoretical progress outside of these nice set of assumptions, it is ultimately where we must progress; it is illogical to use the “best” stochastic gradient method on complex models where the assumptions do not even hold anyways.</p>

<h2 id="references">References</h2>
<ul>
  <li>Rie Johnson and Tong Zhang. Accelerating Stochastic Gradient Descent using Predictive Variance Reduction. <em>NIPS</em>, 2013.</li>
  <li>Nicolas Le Roux, Mark Schmidt, and Francis Bach. A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets. <em>NIPS</em>, 2012.</li>
  <li>Boris T. Polyak and Anatoli Juditsky. Acceleration of stochastic approximation by averaging. <em>SIAM Journal on Control and Optimization</em>, 30(4), 838–855, 1992.</li>
</ul>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
