<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Expectation Conditional Maximization (ECM) and other variants of EM | Dustin Tran</title>
	<meta name="description" content="Here we shall introduce the Expectation Conditional Maximization algorithm (ECM) by Meng and Rubin (1993) by motivating it from a typical example. I’ll also ...">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/expectation-conditional-maximization-ecm-and-other-variants-of-em">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">Expectation Conditional Maximization (ECM) and other variants of EM</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Feb 20, 2015</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><p>Here we shall introduce the Expectation Conditional Maximization algorithm (ECM) by Meng and Rubin (1993) by motivating it from a typical example. I’ll also add some thoughts about other natural considerations at the end.</p>

<p>Let <script type="math/tex">n</script> observations <script type="math/tex">y_1,\ldots,y_n\sim\mathcal{N}(X_i\beta, \Sigma)</script> be sampled i.i.d. where <script type="math/tex">X_i</script>’s are given features for each <script type="math/tex">i</script>. Let both the linear coefficients <script type="math/tex">\beta</script> and the covariance <script type="math/tex">\Sigma</script> be unknown, and say for each <script type="math/tex">y_i</script> only some values are observed, i.e., there exists missing data. Partition the observations <script type="math/tex">Y=(Y_{obs},Y_{mis})</script>, where <script type="math/tex">Y_{obs}</script> represents all those observed and <script type="math/tex">Y_{mis}</script> the missing data.</p>

<p>Now consider the problem of calculating the maximum likelihood estimate (MLE) of <script type="math/tex">\theta=(\beta,\Sigma)</script>. This has no closed form expression, so let’s consider the EM algorithm (Dempster et al., 1977) instead:</p>

<ol>
  <li>Initialize <script type="math/tex">\theta^{(0)}=(\beta^{(0)},\Sigma^{(0)})</script>.</li>
  <li>While <script type="math/tex">\theta</script> has not converged, i.e., <script type="math/tex">\|\theta^{(t+1)}-\theta^{(t)}\|/\|\theta^{(t)}\|\le\epsilon</script>:
    <ol>
      <li>E-step: Calculate expectation of the sufficient statistics, conditional on observed data and current parameter values:</li>
    </ol>

    <p><!--Q(\theta\mid\theta^{(t)}) &= \mathbb{E}_{Y_{mis}}[\ell(\theta; Y)\mid Y_{obs}, \theta=\theta^{(t)}]\\-->
 <script type="math/tex">\mathbb{E}[Y_i\mid Y_{obs}, \beta^{(t)}, \Sigma^{(t)}],\qquad
 \mathbb{E}[Y_iY_i^T\mid Y_{obs}, \beta^{(t)}, \Sigma^{(t)}]</script></p>

    <p>​</p>
    <ol>
      <li>M-step: Substitute the above into expressions for the sufficient statistics</li>
    </ol>

    <script type="math/tex; mode=display">\theta^{(t+1)} = \mathrm{arg\ max}_\theta Q(\theta\mid\theta^{(t)})</script>
  </li>
</ol>

<p>ECM is a natural consideration for EM, which replaces the maximization step over one’s parameters of interest by conditioning on a subset of these parameters</p>

<p>PxEM is a way of incorporating additional information into one’s estimate by modifying the current parameters based on some set of constraints you know must be true.</p>

<p>Intuitively, I see the advantage of this variant in the same way the <a href="">Stein’s estimator</a> dominates the MLE for the trivial example of three observations sampled i.i.d. from a normal <script type="math/tex">y_i\sim \mathcal{N}(\mu_i, 1)</script>, where <script type="math/tex">\mu_i</script>’s are unknown for <script type="math/tex">i=1,2,3</script>. It is easy to see that the MLE is <script type="math/tex">\mu_i^{MLE} = y_i</script>. However, this estimator is inadmissable; it is dominated by the estimate … While this seems paradoxical, what is really happening is the underlying truth that all observations are sampled from a normal distribution with the same variance. By taking advantage of the fact that all observations share this attribute, you can obtain a better estimate by introducing dependencies.</p>

<h2 id="references">References</h2>
<ul>
  <li>Arthur Dempster, Nan Laird, and Donald Rubin. Maximum likelihood from incomplete data via the EM algorithm. <em>Journal of the Royal Statistical Society</em>, Series B 39 (1): 1–38, 1977.</li>
  <li>Xiao-Li Meng and Donald Rubin. Maximum likelihood estimation via the ECM algorithm: a general framework. <em>Biometrika</em>, 80 (2): 267–278, 1993.</li>
</ul>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
