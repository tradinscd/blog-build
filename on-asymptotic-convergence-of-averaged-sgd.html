<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>On asymptotic convergence of averaged SGD | Dustin Tran</title>
	<meta name="description" content="Stochastic gradient descent (SGD) has seen wide application for learning problems on large scale data, whether this be for generalized linear models [6], SVM...">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/on-asymptotic-convergence-of-averaged-sgd">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">On asymptotic convergence of averaged SGD</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Nov 28, 2014</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><p>Stochastic gradient descent (SGD) has seen wide application for learning problems on large scale data, whether this be for generalized linear models [6], SVMs [1], or neural networks [2] alike.  We review a result by Polyak and Juditsky [3] on the theory of performing SGDs with averaged estimates, which shows that it is asymptotically as good as a second-order method.</p>

<p>Recall that in the typical formulation, the problem is to minimize some loss function:</p>

<p><script type="math/tex">\theta^* = {\mathrm{arg\, min}}\, Q(\theta) = \sum_{n=1}^N Q_n(\theta),</script>
<!--$$\theta^* = \underset{\theta}{\mathrm{arg\, min}} Q(\theta) = \sum_{n=1}^N Q_n(\theta),$$--></p>

<p>where <script type="math/tex">Q_n(\theta)</script> is the only part of the function associated with the <script type="math/tex">n^{th}</script> data point. In the statistical formulation, we can formalize this by assuming all <script type="math/tex">N</script> observations are i.i.d., and the loss function corresponds to the negative log-likelihood for some choice of model and its set of parameters <script type="math/tex">\theta=(\theta_1,\ldots,\theta_p)</script>.</p>

<p>Let the learning rate <script type="math/tex">\{\alpha_n\}</script> be a specified decreasing sequence. <em>Stochastic gradient descent</em> uses the update</p>

<script type="math/tex; mode=display">\theta_{n+1} = \theta_n - \alpha_n\nabla Q_{i_n}(\theta_n),</script>

<p>where at each iteration one chooses a randomly sampled data point <script type="math/tex">i_n\in
\{1,\ldots,N\}</script>. This is a stochastic approximation procedure [5], and it is proven to converge to a point where the expected loss <script type="math/tex">\mathbb{E}[\nabla Q(\theta^*)]=0</script>; for convex loss, <script type="math/tex">\theta^*</script> is the global minimum.</p>

<p><strong>Remark</strong>. It was well-known heuristically that one should sample without replacement and make enough passes of the data for SGD, but not well-understood why this is actally better than sampling with replacement. Recht and Re [4] have an interesting proof for this using noncommutative arithmetic-geometric means.</p>

<p>Here is a table which summarizes the most common optimization methods.</p>

<table border="2" style="width:100%; text-align:center;">
<thead>
  <tr>
    <th>method</th>
    <th>Running time complexity</th>
    <th>Convergence rate</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Newton</td>
    <td>$$\mathcal{O}(Np^{2+\epsilon})$$</td>
    <td>quadratic</td>
  </tr>
  <tr>
    <td>Quasi-Newton (e.g. BFGS, DFP)</td>
    <td>$$\mathcal{O}(Np^{1+\epsilon})$$</td>
    <td>superlinear</td>
  </tr>
  <tr>
    <td>Gradient descent</td>
    <td>$$\mathcal{O}(Np^{1-\epsilon})$$</td>
    <td>linear</td>
  </tr>
  <tr>
    <td>Stochastic gradient descent</td>
    <td>$$\mathcal{O}(p)$$</td>
    <td>sublinear</td>
  </tr>
</tbody>
</table>
<p><br /></p>

<p>It’s a clear win for SGD in terms of running time. Newton’s method requires inverting a Hessian <script type="math/tex">[\nabla^2 Q(\theta_n)]^{-1}</script> and computing the full gradient <script type="math/tex">\nabla Q(\theta_n)</script> at each step; Quasi-Newton only slightly relaxes this by using a low rank approximation of the Hessian; and gradient descent doesn’t use it at all but still requires computing <script type="math/tex">\nabla Q(\theta_n)</script>.</p>

<p>However, this comes at the cost of convergence rates. Naturally, SGD can only obtain a sublinear convergence rate <script type="math/tex">\mathcal{O}(1/n)</script> for convex functions, whereas the above can achieve better convergence by using more information per iteration.</p>

<p>While there will always be a point in which SGD surpasses any of the above methods as the size of the data becomes larger (thus causing running time complexity to be the limiting factor), we can still do better than this.</p>

<p>The simplest variant which improves SGD use <em>Polyak-Ruppert averaging</em>:</p>

<script type="math/tex; mode=display">\bar{\theta}_n = \frac{1}{n}\sum_{i=1}^n \theta_i</script>

<p>That is, take the running average of the estimates during each iteration. We define <em>averaged stochastic gradient descent</em> (ASGD) as applying SGD with Polyak-Ruppert averaging.</p>

<p><strong>Proposition</strong>. (Polyak and Judisty [3]) Asymptotically, ASGD is as fast as a second-order SGD, i.e.,</p>

<script type="math/tex; mode=display">\theta_{n+1} = \theta_n - [\nabla^2Q(\theta_n)]^{-1}\nabla Q_{i_n}(\theta_n)</script>

<p>While the convergence rate of a second-order SGD is still sublinear, the constants are much improved. This is reflected in practice [7], and it comes at very little cost due to the simplicity of the small change in implementation.</p>

<p>As I’m currently reviewing the literature on stochastic gradient methods, I’ll post the proof on another page as I think it has a nice derivation.</p>

<h2 id="references">References</h2>

<p>[1] L. Bottou. Online Algorithms and Stochastic Approximations, Online Learning and Neural Networks, Edited by David Saad, <em>Cambridge University Press</em>, 1998.</p>

<p>[2] L. Bottou. Stochastic Gradient Learning in Neural Networks, <em>Proceedings of Neuro-Nîmes 91</em>, EC2, 1991.</p>

<p>[3] B. Polyak and A. Juditsky. Acceleration of stochastic approximation by averaging. <em>Automation and Remote Control</em>, 30(4):838–855, 1992.</p>

<p>[4] B. Recht and C. Re. Beneath the valley of the noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences. <em>preprint</em> <a href="http://arxiv.org/abs/1202.4184">arXiv:1202.4184</a>, 2012.</p>

<p>[5] H. Robbins and S. Monro. A stochastic approximation method. <em>The Annals of Mathematical Statistics</em>, pages 400–407, 1951.</p>

<p>[6] P. Toulis, E. Airoldi, and J. Rennie. Statistical analysis of stochastic gradient methods for generalized linear models. <em>JMLR W&amp;CP</em>, 32(1):667–675, 2014.</p>

<p>[7] W. Xu. Towards optimal one pass large scale learning with averaged stochastic gradient descent. <em>preprint</em> <a href="http://arxiv.org/abs/1107.2490">arXiv:1107.2490</a>, 2011.</p>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
