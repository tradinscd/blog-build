<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>My Qualifying Exam (Oral) | Dustin Tran</title>
	<meta name="description" content="I’m taking my qualifying exam this Tuesday—which may surprise some ofyou that I haven’t already done it! This is mostly due to logisticalkerfuffles as I tran...">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/my-qualifying-exam-oral">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">My Qualifying Exam (Oral)</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Aug 7, 2017</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><p>I’m taking my qualifying exam this Tuesday—which may surprise some of
you that I haven’t already done it! This is mostly due to logistical
kerfuffles as I transferred Ph.D.’s and I also tend to avoid coursework
like the plague.</p>

<p>Each university has its own culture around an oral or qualifying exam.
Columbia’s Computer Science department involves the following:</p>

<blockquote>
  <p>The committee, after consideration of the student’s input, selects a syllabus of the 20-30 most significant documents that encompass the state of the art in the area. […] The oral exam begins with the student’s 30 minute critical evaluation of the syllabus materials, and is followed by no more than 90 minutes of questioning by the committee on any subject matter related to their contents. The student is judged primarily on the oral evidence, but the content and style of the presentation can account for part of the decision.
<a href="http://www.cs.columbia.edu/education/phd/requirements/candidacy/">[url]</a></p>
</blockquote>

<p>My syllabus concerns <em>Bayesian deep learning</em>, which is the
synthesis of modern Bayesian analysis with deep learning.
The syllabus includes 29 papers published in 2014 or later,
representing “the most significant documents that encompass the
state of the art in the area.”
I got multiple requests from friends to share the list, so I decided
to just share it publically.</p>

<p><strong>Probabilistic programming &amp; AI systems</strong></p>
<ol>
  <li><a class="citation" href="#mansinghka2014venture">Mansinghka, Selsam, &amp; Perov (2014)</a></li>
  <li><a class="citation" href="#tristan2014augur">Tristan et al. (2014)</a></li>
  <li><a class="citation" href="#schulman2015gradient">Schulman, Heess, Weber, &amp; Abbeel (2015)</a></li>
  <li><a class="citation" href="#narayanan2016probabilistic">Narayanan, Carette, Romano, Shan, &amp; Zinkov (2016)</a></li>
  <li><a class="citation" href="#abadi2016tensorflow">Abadi et al. (2016)</a></li>
  <li><a class="citation" href="#carpenter2016stan">Carpenter et al. (2016)</a></li>
  <li><a class="citation" href="#tran2016edward">Tran et al. (2016)</a></li>
  <li><a class="citation" href="#kucukelbir2017automatic">Kucukelbir, Tran, Ranganath, Gelman, &amp; Blei (2017)</a></li>
  <li><a class="citation" href="#tran2017deep">Tran et al. (2017)</a></li>
  <li><a class="citation" href="#neubig2017dynet">Neubig et al. (2017)</a></li>
</ol>

<p><strong>Variational inference</strong></p>
<ol>
  <li><a class="citation" href="#kingma2014autoencoding">Kingma &amp; Welling (2014)</a></li>
  <li><a class="citation" href="#ranganath2014black">Ranganath, Gerrish, &amp; Blei (2014)</a></li>
  <li><a class="citation" href="#rezende2014stochastic">Rezende, Mohamed, &amp; Wierstra (2014)</a></li>
  <li><a class="citation" href="#mnih2014neural">Mnih &amp; Gregor (2014)</a></li>
  <li><a class="citation" href="#rezende2015variational">Rezende &amp; Mohamed (2015)</a></li>
  <li><a class="citation" href="#salimans2015markov">Salimans, Kingma, &amp; Welling (2015)</a></li>
  <li><a class="citation" href="#tran2016variational">Tran, Ranganath, &amp; Blei (2016)</a></li>
  <li><a class="citation" href="#ranganath2016hierarchical">Ranganath, Tran, &amp; Blei (2016)</a></li>
  <li><a class="citation" href="#maaloe2016auxiliary">Maaløe, Sønderby, Sønderby, &amp; Winther (2016)</a></li>
  <li><a class="citation" href="#johnson2016composing">Johnson, Duvenaud, Wiltschko, Datta, &amp; Adams (2016)</a></li>
  <li><a class="citation" href="#ranganath2016operator">Ranganath, Altosaar, Tran, &amp; Blei (2016)</a></li>
  <li><a class="citation" href="#gelman2017expectation">Gelman et al. (2017)</a></li>
</ol>

<p><strong>Implicit probabilistic models &amp; adversarial training</strong></p>
<ol>
  <li><a class="citation" href="#goodfellow2014generative">Goodfellow et al. (2014)</a></li>
  <li><a class="citation" href="#dziugaite2015training">Dziugaite, Roy, &amp; Ghahramani (2015)</a></li>
  <li><a class="citation" href="#li2015generative">Li, Swersky, &amp; Zemel (2015)</a></li>
  <li><a class="citation" href="#radford2016unsupervised">Radford, Metz, &amp; Chintala (2016)</a></li>
  <li><a class="citation" href="#mohamed2016learning">Mohamed &amp; Lakshminarayanan (2016)</a></li>
  <li><a class="citation" href="#arjovsky2017wasserstein">Arjovsky, Chintala, &amp; Bottou (2017)</a></li>
  <li><a class="citation" href="#tran2017deepand">Tran, Ranganath, &amp; Blei (2017)</a></li>
</ol>

<p>Committee: David Blei, Andrew Gelman, Daniel Hsu.</p>

<p>Disclaimer: I favored papers which have
shown to be or are most likely to be long-lasting in influence (this
means fewer papers from 2017); papers on methodology rather than
applications (only to narrow the scope); original papers over surveys;
and my own papers (because it’s my oral). If I did not cite you or if
you have strong opinions about a missing paper, recall <a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor">Hanlon’s
razor</a>. E-mail me your
suggestions.</p>

<p>Update (08/08/2017): I passed the oral. :-)</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="abadi2016tensorflow">Abadi, Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … Zhang, X. (2016). TensorFlow: A system for large-scale machine learning, <i>cs.DC</i>, 1–18.</span></li>
<li><span id="arjovsky2017wasserstein">Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). Wasserstein GAN. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="carpenter2016stan">Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., … Riddell, A. (2016). Stan: a probabilistic programming language. <i>Journal of Statistical Software</i>.</span></li>
<li><span id="dziugaite2015training">Dziugaite, G. K., Roy, D. M., &amp; Ghahramani, Z. (2015). Training generative neural networks via Maximum Mean Discrepancy optimization. In <i>Uncertainty in Artificial Intelligence</i>.</span></li>
<li><span id="gelman2017expectation">Gelman, A., Vehtari, A., Jylänki, P., Sivula, T., Tran, D., Sahai, S., … Robert, C. (2017). Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data. <i>ArXiv.org</i>.</span></li>
<li><span id="goodfellow2014generative">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Nets. In <i>Neural Information Processing Systems</i>.</span></li>
<li><span id="johnson2016composing">Johnson, M. J., Duvenaud, D., Wiltschko, A. B., Datta, S. R., &amp; Adams, R. P. (2016). Composing graphical models with neural networks for structured representations and fast inference. In <i>Neural Information Processing Systems</i>.</span></li>
<li><span id="kingma2014autoencoding">Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. In <i>International Conference on Learning Representations</i>.</span></li>
<li><span id="kucukelbir2017automatic">Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., &amp; Blei, D. M. (2017). Automatic Differentiation Variational Inference. <i>Journal of Machine Learning Research</i>, <i>18</i>, 1–45.</span></li>
<li><span id="li2015generative">Li, Y., Swersky, K., &amp; Zemel, R. (2015). Generative Moment Matching Networks. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="maaloe2016auxiliary">Maaløe, L., Sønderby, C. K., Sønderby, S. K., &amp; Winther, O. (2016). Auxiliary Deep Generative Models. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="mansinghka2014venture">Mansinghka, V., Selsam, D., &amp; Perov, Y. (2014). Venture: a higher-order probabilistic programming platform with programmable inference. <i>ArXiv.org</i>.</span></li>
<li><span id="mnih2014neural">Mnih, A., &amp; Gregor, K. (2014). Neural Variational Inference and Learning in Belief Networks. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="mohamed2016learning">Mohamed, S., &amp; Lakshminarayanan, B. (2016). Learning in Implicit Generative Models. <i>ArXiv.org</i>.</span></li>
<li><span id="narayanan2016probabilistic">Narayanan, P., Carette, J., Romano, W., Shan, C.-chieh, &amp; Zinkov, R. (2016). Probabilistic Inference by Program Transformation in Hakaru (System Description). In <i>International Symposium on Functional and Logic Programming</i>. Springer, Cham.</span></li>
<li><span id="neubig2017dynet">Neubig, G., Dyer, C., Goldberg, Y., Matthews, A., Ammar, W., Anastasopoulos, A., … Yin, P. (2017). DyNet: The Dynamic Neural Network Toolkit. <i>ArXiv.org</i>.</span></li>
<li><span id="radford2016unsupervised">Radford, A., Metz, L., &amp; Chintala, S. (2016). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. In <i>International Conference on Learning Representations</i>.</span></li>
<li><span id="ranganath2016operator">Ranganath, R., Altosaar, J., Tran, D., &amp; Blei, D. M. (2016). Operator Variational Inference. In <i>Neural Information Processing Systems</i>.</span></li>
<li><span id="ranganath2014black">Ranganath, R., Gerrish, S., &amp; Blei, D. M. (2014). Black Box Variational Inference. In <i>Artificial Intelligence and Statistics</i>.</span></li>
<li><span id="ranganath2016hierarchical">Ranganath, R., Tran, D., &amp; Blei, D. M. (2016). Hierarchical Variational Models. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="rezende2015variational">Rezende, D. J., &amp; Mohamed, S. (2015). Variational Inference with Normalizing Flows. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="rezende2014stochastic">Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="salimans2015markov">Salimans, T., Kingma, D. P., &amp; Welling, M. (2015). Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. In <i>International Conference on Machine Learning</i>.</span></li>
<li><span id="schulman2015gradient">Schulman, J., Heess, N., Weber, T., &amp; Abbeel, P. (2015). Gradient Estimation Using Stochastic Computation Graphs. In <i>Neural Information Processing Systems</i>.</span></li>
<li><span id="tran2017deep">Tran, D., Hoffman, M. D., Saurous, R. A., Brevdo, E., Murphy, K., &amp; Blei, D. M. (2017). Deep Probabilistic Programming. In <i>International Conference on Learning Representations</i>.</span></li>
<li><span id="tran2016edward">Tran, D., Kucukelbir, A., Dieng, A. B., Rudolph, M., Liang, D., &amp; Blei, D. M. (2016). Edward: A library for probabilistic modeling, inference, and criticism. <i>ArXiv.org</i>.</span></li>
<li><span id="tran2017deepand">Tran, D., Ranganath, R., &amp; Blei, D. M. (2017). Deep and Hierarchical Implicit Models. <i>ArXiv.org</i>.</span></li>
<li><span id="tran2016variational">Tran, D., Ranganath, R., &amp; Blei, D. M. (2016). The Variational Gaussian Process. In <i>International Conference on Learning Representations</i>.</span></li>
<li><span id="tristan2014augur">Tristan, J.-B., Huang, D., Tassarotti, J., Pocock, A. C., Green, S., &amp; Steele, G. L. (2014). Augur: Data-Parallel Probabilistic Modeling. In <i>Neural Information Processing Systems</i> (pp. 2600–2608).</span></li></ol>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
