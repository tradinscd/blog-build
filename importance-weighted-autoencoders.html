<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Importance weighted autoencoders | Dustin Tran</title>
	<meta name="description" content="A new paper on variational autoencoders is out on arXiv, titled “Importance Weighted Autoencoders” by Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov.">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="http://dustintran.com/blog/importance-weighted-autoencoders">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Dustin Tran" href="http://dustintran.com/blog/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
        <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53150914-2', 'auto');
	ga('send', 'pageview');
	</script>
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/blog/">
			<img class="avatar" src="/blog/img/photo.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/blog/">Dustin Tran</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
                        <li>
				<a class="page-link" href="http://dustintran.com">
					about　
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog/')">
    <h1 class="title">Importance weighted autoencoders</h1>
    <div style="padding-top:3%;"></div>
    <div class="meta">
      <span class="author">
        By
        
          <a class="author-name" href="http://dustintran.com">Dustin Tran</a>
        
      </span>
      <span class="date">Sep 5, 2015</span>
      <ul>
        














<li>
	<a href="https://github.com/dustinvtran" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/dustinvtran" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






      </ul>
    </div>
  </header>
  <section class="post-content"><p>A new paper on variational autoencoders is out on arXiv, titled “<a href="http://arxiv.org/abs/1509.00519">Importance Weighted Autoencoders</a>” by Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov.</p>

<p>It’s very well written. As a statistician, I tend to have a hard time understanding the deep learning community’s lingo for variational inference. Simple concepts like what the authors specify as the model likelihood, the model prior, and the variational distribution, are all confuddled. The background section summarizes the recent literature on “black box methods” very well. To be clear, here I define a <em>black box</em> variational inference algorithm as a VI algorithm which can be used for (at least) any continuous latent variable model.</p>

<p>It introduces both paradigms I tend to categorize black box methods: (1) the continuous sort of deep generative models which variational autoencoders are used for and apply a reparameterization trick, and (2) the most general but higher variance methods, which share the common theme of using a REINFORCE gradient.</p>

<p>The main contribution of the paper comes from a simple derivation of importance sampling in the ELBO. With more than one sample, this can lead to a better lower bound on the marginal likelihood. While the derivation is simple, the distinction from a minibatch of samples to estimate the ELBO is very subtle. Here’s the importance weighted ELBO:</p>

<script type="math/tex; mode=display">\mathcal{L}_k(\mathbf{x}) =
\mathbb{E}_{\mathbf{h}_1,\ldots,\mathbf{h}_k\sim q(\mathbf{h}\mid\mathbf{x})}
\left[
\log \frac{1}{k} \sum_{i=1}^k \frac{p(\mathbf{x},\mathbf{h}_i)}{q(\mathbf{h}_i\mid\mathbf{x})}
\right]</script>

<p>Here’s a minibatch ELBO:</p>

<script type="math/tex; mode=display">\mathcal{L}_{minibatch}(\mathbf{x}) =
\frac{1}{k}\sum_{i=1}^k
\left[
\log \frac{p(\mathbf{x},\mathbf{h}_i)}{q(\mathbf{h}_i\mid\mathbf{x})}
\right],
\qquad
\mathbf{h}_1,\ldots,\mathbf{h}_k\sim q(\mathbf{h}\mid\mathbf{x})</script>

<p>Aside from the formulaic difference in the log-addition rather than log-multiplication, the conceptual difference is very clear once you unpack things. The minibatch ELBO is simply a Monte Carlo estimate of the ELBO. That is it estimates the “single sample” $\mathbf{h}\sim q(\mathbf{h}\mid\mathbf{x})$ expectation. The importance weighted ELBO <em>augments</em> the expectation by considering not one but a fixed number of samples from the variational distribution, which is then used to take the expectation with respect to:</p>

<script type="math/tex; mode=display">\mathcal{L}_k(\mathbf{x}) =
\int
q(\mathbf{h}_1\mid\mathbf{x})\cdots\int
q(\mathbf{h}_k\mid\mathbf{x})
\left[
\log \frac{1}{k} \sum_{i=1}^k \frac{p(\mathbf{x},\mathbf{h}_i)}{q(\mathbf{h}_i\mid\mathbf{x})}
\right]
d\mathbf{h}_1\cdots d\mathbf{h}_k</script>

<p>However, it still looks remarkably similar if you use a minibatch of size <script type="math/tex">k</script> to estimate the ELBO and a single set of $k$ samples to estimate the augmented ELBO. In terms of computation, these two are equivalent. It’s not clear to me why the importance weighted ELBO is better.</p>

<p>The paper puts itself into context very well in the related works section. Also is very reminiscent of the recent work of Bornschein &amp; Bengio (2015) on the reweighted wake sleep algorithm (which is also well-written).</p>

<h2 id="references">References</h2>
<ul>
  <li>Jörg Bornschein and Yoshua Bengio. Reweighted Wake-Sleep. <a href="http://arxiv.org/abs/1406.2751"><em>arXiv preprint arXiv:1406.2751</em></a>, 2015.</li>
  <li>Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. <a href="http://arxiv.org/abs/1509.00519"><em>arXiv preprint arXiv:1509.00519</em></a>, 2015.</li>
</ul>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->


    </div>
    
<script src="/blog/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2016 ・ <a class="plain" href="http://dustintran.com">dustintran.com</a>
</p>
</footer>


  </body>
</html>
